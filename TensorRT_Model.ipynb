{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"structure\"></a>\n",
    "### Create Model Directory Structure\n",
    "\n",
    "\n",
    "```\n",
    "root@server:/models$ tree\n",
    ".\n",
    "└── simple-tensorrt-model\n",
    "    ├── 1\n",
    "    │   └── model.plan\n",
    "    └── config.pbtxt\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Below, we'll create the model directory structure for each of our TensorRT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/simple-tensorrt-fp32-model/\n",
    "!mkdir -p models/simple-tensorrt-fp32-model/1/\n",
    "!mkdir -p models/simple-tensorrt-fp16-model/\n",
    "!mkdir -p models/simple-tensorrt-fp16-model/1/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "### Convert ONNX to TensorRT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec \\\n",
    "  --onnx=models/simple-onnx-model/1/model.onnx \\\n",
    "  --explicitBatch \\\n",
    "  --optShapes=actual_input_1:16x3x224x224 \\\n",
    "  --maxShapes=actual_input_1:32x3x224x224 \\\n",
    "  --minShapes=actual_input_1:1x3x224x224 \\\n",
    "  --shapes=actual_input_1:1x3x224x224 \\\n",
    "  --saveEngine=models/simple-tensorrt-fp32-model/1/model.plan\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert our ONNX representation to a TensorRT plan, we'll point to our `model.onnx` file and specify the output for our newly created TensorRT plan. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the `--fp16` flag, we can specify that our TensorRT plan will be optimized for FP16. There are a lot of benefits to using FP16, mainly of which is that it is faster (fewer computations) and uses less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec \\\n",
    "  --onnx=models/simple-onnx-model/1/model.onnx \\\n",
    "  --explicitBatch \\\n",
    "  --optShapes=actual_input_1:16x3x224x224 \\\n",
    "  --maxShapes=actual_input_1:32x3x224x224 \\\n",
    "  --minShapes=actual_input_1:1x3x224x224 \\\n",
    "  --shapes=actual_input_1:1x3x224x224 \\\n",
    "  --saveEngine=models/simple-tensorrt-fp16-model/1/model.plan \\\n",
    "  --fp16\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configuration\"></a>\n",
    "### Create Configuration File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"simple-tensorrt-fp32-model\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    " {\n",
    "    name: \"actual_input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/simple-tensorrt-fp32-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a configuration file for the TensorRT Fp16 model. Note that our input and output data types still remain in their FP32 representation - the internal layers and activations of our neural network will use the FP16 data type but our input and output data will still be in FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"simple-tensorrt-fp16-model\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    " {\n",
    "    name: \"actual_input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/simple-tensorrt-fp16-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "### Load Model in Triton Inference Server\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTTP request returns status 200 if Triton is ready and non-200 if it is not ready.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/simple-tensorrt-fp32-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/models/simple-tensorrt-fp16-model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"infer\"></a>\n",
    "### Send Inference Request to Server\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "from tritonclient.utils import triton_to_np_dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./imagenet-simple-labels.json') as file:\n",
    "    labels = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "img_path = './assets/goldfish.jpg'\n",
    "image_pil = Image.open(img_path)\n",
    "image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "image_numpy = image.img_to_array(img)\n",
    "image_numpy = np.expand_dims(image_numpy, axis=0)\n",
    "image_numpy = preprocess_input(image_numpy)\n",
    "image_numpy = np.transpose(image_numpy, [0, 3, 1, 2]) / 255.\n",
    "print(image_numpy.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the input and output names of our model, the name of our model, the URL where our models are deployed with Triton Inference Server (in this case local host of `triton:8000`), and our model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = 'actual_input_1'\n",
    "output_name = 'output1'\n",
    "model_name = 'simple-tensorrt-fp32-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = tritonhttpclient.InferInput(input_name, (1, 3, 224, 224), 'FP32')\n",
    "input0.set_data_from_numpy(image_numpy, binary_data=False)\n",
    "\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "response = triton_client.infer(model_name, model_version=model_version, \n",
    "                               inputs=[input0], outputs=[output])\n",
    "logits = response.as_numpy('output1')\n",
    "logits = np.asarray(logits, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[np.argmax(logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = FIXME\n",
    "input_name = FIXME\n",
    "input_shape = FIXME\n",
    "input_dtype = FIXME\n",
    "output_name = FIXME\n",
    "model_name = FIXME\n",
    "url = FIXME\n",
    "model_version = FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "input_name = 'actual_input_1'\n",
    "input_shape = (1, 3, 224, 224)\n",
    "input_dtype = 'FP32'\n",
    "output_name = 'output1'\n",
    "model_name = 'simple-tensorrt-fp16-model'\n",
    "url = 'triton:8000'\n",
    "model_version = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client = FIXME\n",
    "model_metadata = FIXME\n",
    "model_config = FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_client = tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
